{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02f1faa",
   "metadata": {},
   "source": [
    "# Text Generation with Transformers in TensorFlow \n",
    "# Simple_Tranformer_Language_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2588532",
   "metadata": {},
   "source": [
    "This example demonstrates how to use the `transformers` library by Hugging Face to perform text generation with a pre-trained GPT-2 model in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa1af0",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "- Python 3.x\n",
    "- `transformers` library\n",
    "- `tensorflow` library\n",
    "\n",
    "You can install the required libraries using pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e4477",
   "metadata": {},
   "source": [
    "### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1207a421",
   "metadata": {},
   "source": [
    "#### 1. Importing Necessary Libraries\n",
    "First, we import the necessary libraries from the `transformers` package and TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e623caff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec14d7e",
   "metadata": {},
   "source": [
    "- `AutoTokenizer`: A utility from the `transformers` library to handle tokenization.\n",
    "- `TFGPT2LMHeadModel`: The TensorFlow version of GPT-2 model for language modeling tasks.\n",
    "- `tensorflow as tf`: TensorFlow library for tensor manipulation and running the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fb066",
   "metadata": {},
   "source": [
    "#### 2. Initializing the Tokenizer and Model\n",
    "We initialize the tokenizer and model with the pre-trained `distilgpt2` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b4bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"distilgpt2\", output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc76375",
   "metadata": {},
   "source": [
    "- `AutoTokenizer.from_pretrained('distilgpt2')`: Loads the pre-trained tokenizer for `distilgpt2`.\n",
    "- `TFGPT2LMHeadModel.from_pretrained('distilgpt2', output_hidden_states=True)`: Loads the pre-trained GPT-2 model with an option to output hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf06ff8",
   "metadata": {},
   "source": [
    "#### 3. Tokenizing the Input Text\n",
    "We tokenize the input text \"The chicken didn't cross the road because it was\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd547a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input text\n",
    "text = \"The chicken didn't cross the road because it was\"\n",
    "\n",
    "# Tokenize the input string\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"tf\")\n",
    "attention_mask = tf.ones_like(input_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d85faa",
   "metadata": {},
   "source": [
    "- `tokenizer.encode(text, return_tensors='tf')`: Converts the input text into token IDs and returns a TensorFlow tensor.\n",
    "- `tf.ones_like(input_ids)`: Creates an attention mask tensor with the same shape as `input_ids`, filled with ones. This mask indicates which tokens should be attended to (all in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7591b6",
   "metadata": {},
   "source": [
    "#### 4. Setting Special Tokens for Generation\n",
    "We set the `pad_token_id` to the `eos_token_id` to handle padding correctly during generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411290bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set pad_token_id to eos_token_id for open-end generation\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feeb658",
   "metadata": {},
   "source": [
    "- `pad_token_id`: The token ID used for padding sequences to the same length.\n",
    "- `eos_token_id`: The token ID for the end-of-sequence token. Setting `pad_token_id` to `eos_token_id` helps in managing the end of generation sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1301d6b2",
   "metadata": {},
   "source": [
    "#### 5. Generating Text\n",
    "We generate text using the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16d9454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\walte\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\tf_utils.py:728: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the model\n",
    "output = model.generate(input_ids, attention_mask=attention_mask, max_length=20, do_sample=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049991fd",
   "metadata": {},
   "source": [
    "- `model.generate(...)`: Generates text based on the input tensor.\n",
    "  - `input_ids`: The tokenized input text.\n",
    "  - `attention_mask`: The mask indicating which tokens should be attended to.\n",
    "  - `max_length=20`: The maximum length of the generated sequence.\n",
    "  - `do_sample=True`: Indicates whether sampling should be used for text generation (introduces randomness)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026aba87",
   "metadata": {},
   "source": [
    "#### 6. Decoding and Printing the Output\n",
    "Finally, we decode the generated token IDs back into a readable string and print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e44373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The chicken didn't cross the road because it was too soft so I decided to take it to T\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the output\n",
    "print('\\n', tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e903c",
   "metadata": {},
   "source": [
    "- `tokenizer.decode(output[0], skip_special_tokens=True)`: Converts the generated token IDs back into a human-readable string, skipping special tokens (e.g., padding or end-of-sequence tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdefc8d9",
   "metadata": {},
   "source": [
    "### Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d03a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The chicken didn't cross the road because it was close enough to the front of the road, it\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"distilgpt2\", output_hidden_states=True)\n",
    "\n",
    "# Input text\n",
    "text = \"The chicken didn't cross the road because it was\"\n",
    "\n",
    "# Tokenize the input string\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"tf\")\n",
    "attention_mask = tf.ones_like(input_ids)\n",
    "\n",
    "# Set pad_token_id to eos_token_id for open-end generation\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Run the model\n",
    "output = model.generate(input_ids, attention_mask=attention_mask, max_length=20, do_sample=True)\n",
    "\n",
    "print('\\n', tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643cdd3b",
   "metadata": {},
   "source": [
    "### How to Use\n",
    "1. Install the required libraries.\n",
    "2. Run the complete code block provided above.\n",
    "3. Modify the `text` variable to test the model with different input texts.\n",
    "4. Adjust the `max_length` parameter to control the length of the generated text.\n",
    "5. Set `do_sample` to `False` if you want deterministic output (without randomness)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55163a",
   "metadata": {},
   "source": [
    "This code snippet demonstrates the basics of text generation using a pre-trained GPT-2 model in TensorFlow, showcasing how to handle tokenization, model configuration, and generation outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
